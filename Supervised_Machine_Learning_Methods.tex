\documentclass[aspectratio=169]{beamer}
\usepackage{adjustbox}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{multimedia}
\usepackage{ulem}
\usepackage{wasysym}
\usetikzlibrary{mindmap,trees}

\usepackage{pbox}

\usepackage[absolute,overlay]{textpos}

% \usetikzlibrary{mindmap,trees}

\usepackage{smartdiagram}
\usetikzlibrary{shapes.geometric,calc}
\usetikzlibrary{shapes.symbols}
\usetikzlibrary{shapes.symbols,positioning}
\usepackage{metalogo}

\usetikzlibrary{backgrounds, calc, shadows, shadows.blur}

\newcommand\addcurlyshadow[2][]{
    % #1: Optional aditional tikz options
    % #2: Name of the node to "decorate"
    \begin{pgfonlayer}{background}
        \path[blur shadow={shadow xshift=0pt, shadow yshift=0pt, shadow blur steps=6}, #1]
        ($(#2.north west)+(.3ex,-.5ex)$)
        -- ($(#2.south west)+(.5ex,-.7ex)$)
        .. controls ($(#2.south)!.3!(#2.south west)$) .. (#2.south)
        .. controls ($(#2.south)!.3!(#2.south east)$) .. ($(#2.south east)+(-.5ex,-.7ex)$)
        -- ($(#2.north east)+(-.3ex, -.5ex)$)
        -- cycle;
   \end{pgfonlayer}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Style modifications
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usetheme{Berlin}

%%% Fonts %%%

% Change font. Fontspec requires xelatex instead of pdflatex!
% Font catalog: https://www.tug.dk/FontCatalogue/
\usepackage{fontspec}

%\setsansfont{Comfortaa}
%\setsansfont{DejaVu Sans}
%\setsansfont{Fira Sans}

% Use "Fira Sans Light" as the normal font and the "Fira Sans" for
% bold fonts

\setsansfont[
  ItalicFont={Fira Sans Light Italic},
  BoldFont={Fira Sans},
  BoldItalicFont={Fira Sans Italic}]{Fira Sans Light}

\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}

%%% Slide template %%%

\setbeamertemplate{frames}[default]

% Empty headline / footline
\setbeamertemplate{headline}{}
\setbeamertemplate{footline}{}

% Remove navigation icons
\setbeamertemplate{navigation symbols}{}

%%% Colors %%%

\usecolortheme{crane}

\definecolor{lightgray}{RGB}{220,220,220}
\definecolor{darkgray}{RGB}{45,45,45}
%\definecolor{darkblue}{RGB}{0,86,137}
\definecolor{darkblue}{RGB}{22,90,151} % Blue of the Gießen Website
\definecolor{lightblue}{RGB}{229, 245, 255}

\definecolor{darkblue}{RGB}{208,29,44}


% Use the slide background in block environments                                                                                                      
\setbeamercolor{title}{fg=white,bg=darkblue}
\setbeamertemplate{blocks}[default]
\setbeamercolor{block title}{bg=}
\setbeamercolor{block body}{bg=lightgray}
\setbeamercolor{frametitle}{fg=white,bg=darkblue}
\setbeamerfont{block body}{size=\large}
\setbeamercolor{itemize item}{fg=black}
\setbeamertemplate{itemize items}[circle]
\setbeamercolor{section number projected}{bg=darkblue,fg=white}
\setbeamercolor{section in toc}{fg=black}
\setbeamercolor{subsection in toc}{fg=darkblue}

\addtobeamertemplate{block begin}{\pgfsetfillopacity{0.8}}{\pgfsetfillopacity{1}}
\addtobeamertemplate{frametitle}{\pgfsetfillopacity{1.0}}{\pgfsetfillopacity{1}}
\addtobeamertemplate{title page}{\pgfsetfillopacity{1.0}}{\pgfsetfillopacity{1}}

% Command to place the test (e.g. citation) in the center of the footer
\newcommand{\setfootercentertext}[1]{
  \setbeamertemplate{footline}{
    \hspace*{\fill}
    \raisebox{3mm}[0mm][0mm]{
      \tiny{#1}}\hspace*{\fill}}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Content
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%------------------------------------------------------------------------------
\title{Supervised Machine Learning Methods}
%------------------------------------------------------------------------------

\author{\small Konrad U. Förstner}

\institute{ZB MED -- Information Centre for Life Science \& TH Köln}

\date{\scriptsize 
  Workshop \textit{Systems Biology: From large datasets to biological insight}\\
  at the European Bioinformatics Institute\\
  2019-07-10\\\ \\
  % \includegraphics[width=1.0cm]{images/vcard_qr_code.png}\ \\ \ \\
  %\includegraphics[width=0.88cm]{images/creative_commons_attribute.png}
}

\logo{
  \includegraphics[height=1.0cm]{images/ZBMED_2017_EN.pdf}
  \includegraphics[height=0.8cm]{images/logo_TH-Koeln_CMYK_22pt.eps}
}

\begin{document}

%\setbeamertemplate{background}{
  % \includegraphics[height=\paperheight]{images/flickr_normanbleventhalmapcenter_2710796662_mod.jpg}
%}

\begin{frame}{}
  \titlepage
\end{frame}
\logo{}

\begin{frame}{}
  \frametitle{Small survey before we start}

  \begin{block}{}
    \begin{center}    
      Who is already familiar with ...
      \begin{itemize}
      \item ... supervised machine learning -
        classification and regression?
      \item ... Python?
      \item ... Jupyter Notebooks?
      \item ... scikit-learn?
      \end{itemize}
    \end{center}
  \end{block}

\end{frame}

\setcounter{tocdepth}{1}
\begin{frame}{}
   \tableofcontents
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{}
   \tableofcontents[currentsection]
\end{frame}

\begin{frame}

  \begin{center}
    \begin{adjustbox}{max totalsize={.9\textwidth}{.7\textheight},center}
    \begin{tikzpicture}
      \path[mindmap,concept color=red,text=white]
      node[concept] {Machine\\learning}
      [clockwise from=110]
      %% child[concept color=green!50!black] {
      %%   node[concept] {practical}
      %%   [clockwise from=90]
      %%   child { node[concept] {algorithms} }
      %%   child { node[concept] {data structures} }
      %%   child { node[concept] {pro\-gramming languages} }
      %%   child { node[concept] {software engineer\-ing} }
      %% }  
      %% child[concept color=blue] {
      %%   node[concept] {applied}
      %%   [clockwise from=-30]
      %%   child { node[concept] {databases} }
      %%   child { node[concept] {WWW} }
      %% }
      %child[concept color=red] { node[concept] {Supervised learning}}
      
      %child[concept color=orange] { node[concept] {Unsupervised learning}}
        %
        %child[concept color=red] { node[concept] {Dimension reduction}}

        %child [concept color=red] { node[concept] {Regression}} 
        %child [concept color=red] { node[concept] {Classification}}
      
      child[concept color=orange] { node[concept] {Supervised learning}}
      child[concept color=orange] { node[concept] {Unsuper\-vised learning}}
      child[concept color=orange] { node[concept] {Rein\-forcement learning}}

      ;
    \end{tikzpicture}  
    \end{adjustbox}
  \end{center}
  
\end{frame}

\begin{frame}
  \frametitle{Basic concept of supervised machine learning}

  \begin{block}{}
      \begin{center}
      Supervised learning means to generalize from given examples.\\
      \ \\
      Mapping from input X (two-dimensional matrix) to output vectore y\\
      (label for classification, values for regression).\\
      \ \\
      
      $X_{1} \rightarrow y_{1}$\\
      $X_{2} \rightarrow y_{2}$\\
      $X_{3} \rightarrow y_{3}$\\

      \ \\
      Generating a function to map input variables ($X$) to an output
      variable $y$. \\
      \ \\
      $y = f(X)$
      \end{center}
    \end{block}
\end{frame}

\begin{frame}
  \frametitle{Two tasks that can be solved with supervised learning}
  \begin{center}
    \includegraphics[width=13cm]{images/classification_and_regression.pdf}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Classification types}
  \begin{center}
    \includegraphics[width=13cm]{images/binary_vs_multi-class_classification.pdf}
  \end{center}  
\end{frame}

\begin{frame}
  \frametitle{Think-Pair-Share}
  \begin{block}{}
    \begin{center}
      Where in your research or your daily life have you\\
      experienced supervised machine learning?
    \end{center}    
  \end{block}
\end{frame}

%% \begin{frame}
%%   \frametitle{Basic workflow of supevised machine learning}
%%   \begin{center}
%%     % training - estimating the parameters
%%     % test
%%   \end{center}  
%% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%
\section{Foundations and terms}
%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{}
   \tableofcontents[currentsection]
\end{frame}

\begin{frame}
  \frametitle{Entities and their features}
  \begin{block}{}
    \begin{center}
      \begin{itemize}
      \item Entities (aka. samples or data points) are described by
        features (aka.  covariates or attributes).
      \item Feature can be
        \begin{itemize}
        \item categorical
          \begin{itemize}
          \item Nominal (e.g. eye color, gender)
          \item Ordinal (e.g. very bad, bad, good, very good)
          \end{itemize}
        \item numerical
          \begin{itemize}
          \item Discrete (e.g. gene length in nucleotide)
          \item Continous (e.g. body length)
          \end{itemize}
        \end{itemize}
      \end{itemize}
    \end{center}
  \end{block}    
\end{frame}

\begin{frame}
  \frametitle{Features}
  \begin{block}{}
    \begin{center}
      \textbf{Feature selection}: Choosing the most descriptive
      features.
    \end{center}  
  \end{block}

  \begin{block}{}
    \begin{center}
      \textbf{Feature encoding}: Translating categorial values into
      numerical values.
    \end{center}  
  \end{block}  

  \begin{block}{}
    \begin{center}
      \textbf{Feature scaling}: Normalize feature values (e.g. scale
      by minimal and maximum values)
    \end{center}  
  \end{block}  
  
\end{frame}

\begin{frame}
  \frametitle{How well does the model fit?}

  \begin{block}{}
    \begin{center}
      \textbf{Overfitting}: Good performance on the training data, poor
      generliazation to other data.\\      
    \end{center}
  \end{block}
  \begin{block}{}
    \begin{center}
      \textbf{Underfitting}: Poor performance on the training data and
      poor generalization to other data\\
    \end{center}
  \end{block}
  
  \begin{block}{}
    \begin{center}
      \textbf{Regularization}: Different methods to prevent overfitting\\
    \end{center}
  \end{block}
  
\end{frame}

%% \begin{frame}
%%   \frametitle{Evaluation of classificaions}

%%   Evaluation of binary classification - Confusion matrix for binary classification
  
%%   \begin{itemize}
%%   \item Positive (P): Observation is positive (for example: is an apple)
%%   \item Negative (N): Observation is not positive (for example: is not an apple).
%%   \item True Positive (TP): Observation is positive, and is predicted to be positive.
%%   \item False Negative (FN): Observation is positive, but is predicted negative.
%%   \item True Negative (TN): Observation is negative, and is predicted to be negative.
%%   \item False Positive (FP): Observation is negative, but is predicted positive.

%%   \item Accuracy
%%     ACC = (TP + TN) / (P + N)

%%   \item Recall sensitivity,  true positive rate
%%     TPR = TP / P

%%   \item Precision /  positive predictive value
%%     PPV = TP / (TP + FP)

%%   \item F1 (aka F-measure, F-score) - harmonic mean of precision and recall

%%     F1 = 2 * (1 / (1/recall) + (1/recall))
%%   \end{itemize}
  
%% \end{frame}


\begin{frame}
  \frametitle{Overview of different methods}
  \begin{block}{}
    \begin{center}
      \begin{itemize}
      \item Nearest Neighbor
      \item Naive Bayes
      \item Linear regression
      \item Logistic regression
      \item Decision Trees
      \item Artificial neural network (multilayer perceptron)
      \item Genetic Programming
        % \item Ensemble Methods
      \end{itemize}
    \end{center}    
  \end{block}
\end{frame}


%% \begin{frame}
%%   \begin{center}
%%     \begin{block}{}
%%       What is the best method?\\
%%       \pause
%%       It depends.
%%     \end{block}
%%   \end{center}    
%% \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Selected supervised learning methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{}
   \tableofcontents[currentsection]
\end{frame}

%-------------------------------
\subsection{k-Nearest Neighbors}
%-------------------------------

\begin{frame}
  \frametitle{k-Nearest Neighbors}
  \begin{block}{}
    \begin{center}
      \begin{itemize}
      \item For classification and regression
      \item Simplest case of supervised machine learning
      \item Can be easily applied to multi-class classification
      \end{itemize}
    \end{center}
  \end{block}    
\end{frame}

\begin{frame}
  \frametitle{k-Nearest Neighbors}
  \begin{center}
    \includegraphics[width=13.0cm]{images/k_nearest_neighbour_classification_only_training_data.pdf}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{k-Nearest Neighbors}
  \begin{center}
    \includegraphics[width=13.0cm]{images/k_nearest_neighbour_classification_k_1.pdf}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{k-Nearest Neighbors}
  \begin{center}
    \includegraphics[width=13.0cm]{images/k_nearest_neighbour_classification_k_3.pdf}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{k-Nearest Neighbors}
  \begin{center}
    \includegraphics[width=13.0cm]{images/k_nearest_neighbour_regression_only_training_data.pdf}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{k-Nearest Neighbors}
  \begin{center}
    \includegraphics[width=13.0cm]{images/k_nearest_neighbour_regression_k_1.pdf}    
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{k-Nearest Neighbors}
  \begin{center}
    \includegraphics[width=13.0cm]{images/k_nearest_neighbour_regression_k_3.pdf}
  \end{center}
\end{frame}


%-------------------------
\subsection{Linear models}
%-------------------------


%% \begin{frame}
%%   \frametitle{}

%%   Linear regression finds the parameters w and b that minimize the
%%   mean squared error between predictions and the true regression
%%   targets, y, on the training set. The mean squared error is the sum
%%   of the squared differences between the predictions and the true
%%   values.\\

%%   The Model is a line (a single feature), a plane (two feature) or a
%%   hyperplane (three or more features)
%% \end{frame}

\begin{frame}
  \frametitle{Linear models}
  \begin{block}{}
    \begin{center}
      $\hat{y} = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + ... + w_{n}x_{n} + b$\\
      \ \\
      with $n$ as the  number of features\\
      $w$ are the different weights/coefficients\\
      $b$ the intercept\\
    \end{center}
  \end{block}  
\end{frame}

\begin{frame}
  \frametitle{Different ways to estimate the parameters}
  \begin{block}{}
    \begin{center}
      \begin{itemize}
      \item Ordinary Least Squares
        \begin{itemize}
        \item no parameters - easy to use but no possibility to adapt
        \end{itemize}
      \item Ridge
        \begin{itemize}
        \item coefficients should be close to zero
        \item more resistant against overfitting
        \end{itemize}    
      \item Least Absolute Shrinkage and Selection Operator (LASSO)
      \end{itemize}
    \end{center}
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Ordinary least squares (OLS)}
  \begin{center}
    \includegraphics[width=13.0cm]{images/linear_model_ordinary_least_squares.pdf}
    Minimize the offset between $\hat{y}$ and $y$ the\\
    mean squared error (MSE) or sum of squared errors (SSE).
  \end{center}
\end{frame}


\begin{frame}
  \frametitle{}
  \begin{block}{}
    \begin{center}
      Once the parameters ($b$ and the weights $w$) of\\
      \ \\
      $\hat{y} = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + ... + w_{n}x_{n} + b$\\
      \ \\
      are estimated the prediction can be performed\\
      by putting the x values of the data points into the\\
      equation to predict the y value.
    \end{center}
  \end{block}
\end{frame}



%% \begin{frame}
%%   \frametitle{Linear models for binary classification}

%%   Linear models can be used for binary classification by
%%   decision boundary

%%   Different ways to estimate the parameters
%%   \begin{itemize}
%%   \item Logistic regression (*)
%%   \item Linear support vector machines
%%   \end{itemize}
  
%%   (* yes, a classificatio method although called regression)
  
%% \end{frame}
  
%%   decision boundary
%%   \begin{itemize}
%% \item decision boundary (line, plane, hyperplane)
%% \item different algoristhms available - most common once:
%% \begin{itemize}
%% \item logistic regression
%% \item linear SVMs)
%% \end{itemize}

%% \begin{frame}
%%   \frametitle{Advantages and disadvantages}
%%   \begin{itemize}
%%   \item Easy to understand
%%   \item Fast to train
%%   \item Linear regression has no parameters to choose - easy to use but also
%%     not way to control model complexity
%%   \item Only for linear problems
%%   \end{itemize}
%% \end{frame}

%-----------------------------------------
\subsection{Support Vector Machine (SVMs)}
%-----------------------------------------

%% \begin{frame}
%%   \frametitle{Support Vector Machine (SVMs)}
%%   \begin{itemize}
%%   \item For regression
%%   \item For binary classification
%%   \end{itemize}
%% \end{frame}

%% \begin{frame}
%%   \frametitle{Key concept}

%% \begin{itemize}
%% \item Hyperplane
%%   \begin{itemize}
%%   \item The general term for a straight line in a high-dimensional space is a hyperplane
%%   \end{itemize}
%% \item Soft margin
%%   \begin{itemize}
%%   \item introducing the soft margin necessitates introducing a
%%     user-specified parameter that controls, roughly, how many examples
%%     are allowed to violate the separating hyperplane and how far across
%%     the line they are allowed to go.
%%   \item allows some data points to push their way through the margin of the
%%     separating hyperplane without affecting the final result
%%   \item the soft margin parameter specifies a trade-off between hyper-plane
%%     violations and the size of the margin.
%%   \end{itemize}
%% \item Kernel
%%   \begin{itemize}
%%   \item able to handle non-linear data
%%   \item can be used on data a that are not vectors (important for biological data DNA, Protein etc.)
%%   \item able to combine data of different types (e.g. transcriptomics and proteomics data)
%%   \end{itemize}
%% \item Kernel function
%%   \begin{itemize}
%%   \item It is possible to prove that, for any given data set with
%%     consistent labels (where consistent simply means that the data set
%%     does not contain two identical objects with opposite labels) there
%%     exists a kernel function that will allow the data to be linearly
%%     separated.
%%   \end{itemize}
%% \end{itemize}

\begin{frame}
  \frametitle{SVM -- Separeting hyperplan}
  \begin{center}
    \includegraphics[width=13.0cm]{images/svm_potential_separating_hyperplanes.pdf}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{SVM -- Margin}
  \begin{center}
    \includegraphics[width=13.0cm]{images/svm_with_margin.pdf}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{SVM -- Soft Margin}
  \begin{center}
    \includegraphics[width=13.0cm]{images/svm_with_soft_margin.pdf}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{SVM -- Kernel trick}
  \begin{center}
    \includegraphics[width=13.0cm]{images/svm_kernel_trick_1.pdf}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{SVM -- Kernel trick}
  \begin{center}
    \includegraphics[width=13.0cm]{images/svm_kernel_trick_2.pdf}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{SVM -- Kernel trick}
  \begin{center}
    \includegraphics[width=13.0cm]{images/svm_kernel_trick_3.pdf}
  \end{center}
\end{frame}

%\section{Multiclass classification}

%% \begin{itemize}
%% \item Combining several SVMs e.g. train multiple one-versus-all classifiers
%% \end{itemize}
%% \end{frame}


%-------------------------
\subsection{Decision trees and Random forest}
%-------------------------

\begin{frame}
  \frametitle{Decision Trees}
  \begin{center}
    \includegraphics[width=13.0cm]{images/decision_tree_1.pdf}
  \end{center}  
\end{frame}

\begin{frame}
  \frametitle{Decision Trees}
  \begin{center}
    \includegraphics[width=13.0cm]{images/decision_tree_2.pdf}
  \end{center}  
\end{frame}

%% \begin{frame}
%%   Decision trees
%%   \begin{itemize}
%%   \item Concepts:
%%   \item node
%%   \item edge
%%   \item root
%%   \item leaf
%%   \item child
%%   \item parent
%%   \item A forest is a set of n ≥ 0 disjoint trees
%%   \item features: real-valued or categorial and also missing values
%%   \end{itemize}
%% \end{frame}

\begin{frame}
  \frametitle{Random forest}
  \begin{block}{}
    \begin{center}
      \begin{itemize}
      \item In the random forests approach, many different decision trees
        are grown by a randomized tree-building algorithm.
      \item The training set is sampled with replacement to produce a
        modified training set of equal size to the original but with some
        training items included more than once.
      \item In addition, when choosing the question at each node, only a
        small, random subset of the features is considered.
      \item Random decision trees from subsets of features or data point
      \end{itemize}
    \end{center}
  \end{block}
\end{frame}

%% \begin{frame}
%%   \frametitle{Advantages and disadvantages}
%%   \begin{itemize}
%%   \item Advantages:
%%     \begin{itemize}
%%     \item Decision of classification is transparent / interpretable (Better understandable than SVM / neural networks)
%%     \item Possible to label more than two classes
%%     \item Fast
%%     \end{itemize}
%%   \item Disadvantages:
%%     \begin{itemize}
%%     \item Overfitting! "A crucial aspect to applying decision trees is
%%       limiting the complexity of the learned trees so that they do not
%%       overfit the training examples" => Cross-validation on left-out
%%       training examples should be used to ensure that the trees
%%       generalize beyond the examples used to construct them.
%%     \end{itemize}
%%   \end{itemize}    
%% \end{frame}

%--------------------------------------
\subsection{Artificial Neural Networks}
%--------------------------------------

\begin{frame}
  \begin{block}{}
    \begin{center}
      \frametitle{Artificial Neural Networks}
      \begin{itemize}
      \item aka. Multilayer perceptrons or Feed-forward neural networks
      \item Inspired by natural Neural Networks
      \item For classification or regression
      \end{itemize}
    \end{center}  
  \end{block}  
\end{frame}


\begin{frame}
  \frametitle{Artificial Neural Networks}
  \begin{center}    
    \includegraphics[width=13.0cm]{images/ANN_without_hidden_layer.pdf}
  \end{center}  
\end{frame}

\begin{frame}
  \frametitle{Artificial Neural Networks}
  \begin{center}
    \includegraphics[width=13.0cm]{images/ANN_with_hidden_layer.pdf}
  \end{center}  
\end{frame}

%%   Alternative Names:
%%   \begin{itemize}
%%   \item 
%%   \item 
%%   \end{itemize}
%% 

%% \begin{frame}
%%   \frametitle{Basics}
%%   \begin{itemize}
%%   \item "A feedforward neural network, aka multi-layer perceptron
%%     (MLP), is a series of logistic regression models stacked on top of
%%     each other, with the final layer being either another logistic
%%     regression or a linear regression model, depending on whether we
%%     are solving a classification or regression problem." (Murphy)

%%   \item "MLPs can be viewed as generalizations of linear models that
%%     perform multiple stages of processing to come to a decision."

%%   \item ŷ = w$_{\text{0}}$ * x$_{\text{0}}$ + w$_{\text{1}}$ *
%%     x$_{\text{1}}$ + \ldots{} + w$_{\text{p}}$ * x$_{\text{p}}$ + b

%%     ŷ is a weighted sum of the input features x$_{\text{0}}$ to
%%     x$_{\text{p}}$, weighted by the learned coefficients w$_{\text{0}}$ to w$_{\text{p}}$

%%   \item This model has a lot more coefficients (also called weights)
%%     to learn: there is one between every input and every hidden unit
%%     (which make up the hidden layer), and one between every unit in
%%     the hidden layer and the output.

%%   \item coefficient = weights

%%   \item rectifying nonlinearity (also known as rectified linear unit
%%     or relu) or the tangens hyperbolicus (tanh).

%%   \item actication function - e.g sigmoid function

%%   \item weight on edges
%%   \item bias on nodes
%%   \end{itemize}
%% \end{frame}

%% \begin{frame}
%%   \frametitle{Perceptrons}

%%   \begin{itemize}
%%   \item the simplest possible neural network - a single neuron with
%%     one or more inputs and a processor and a single output
%%   \item feed-forward
%%   \item weights of the inputs
%%   \end{itemize}
  
%% \end{frame}

%% \begin{frame}
%%   \frametitle{Multi-layer perceptron or a feed-forward network}

%%   \begin{itemize}
%%   \item "Feed-forward neural networks can also be used for regression
%%     problems, which require continuous outputs, as opposed to binary
%%     outputs (0 and 1)"

%% "By replacing the step function with a continuous function, the
%% neural network outputs a real number."
%% \end{itemize}
%% \end{frame}

%% \begin{frame}
%%   \frametitle{Backpropagation}
%% \end{frame}

%% \begin{frame}
%%   \frametitle{Parameters influence neural network compelexity}

%%   \begin{itemize}
%%   \item the number of hidden layers
%%   \item the number of perceptrons in each hidden layer
%%   \item the regularization
%%   \item and others

%%   \item "An important property of neural networks is that their weights are
%%     set randomly before learning is started, and this random
%%     initialization affects the model that is learned. That means that
%%     even when using exactly the same parameters, we can obtain very
%%     different models when using different random seeds. If the networks
%%     are large, and their complexity is chosen properly, this should not
%%     affect accuracy too much, but it is worth keeping in mind
%%     (particularly for smaller networks)."
%%   \end{itemize}
%% \end{frame}

%% \begin{frame}
%%   \frametitle{Cost Function / Gradient decent}
%%   \begin{itemize}
%%   \item each node can be simple seen as a function
%%   \end{itemize}  
%% \end{frame}

%% \begin{frame}
%%   \frametitle{Classis example - hand written numbers}
%%   \begin{itemize}
%%   \item MNIST database (Modified National Institute of Standards and Technology database)
%%   \item 28 x 28 pixels
%%   \item 60,000 training images and 10,000 testing images
%%   \end{itemize}
%% \end{frame}

%% \begin{frame}
%%   \frametitle{Deep Learning}
%% \end{frame}

%% %An introduction to deep learning on biological sequence data: examples and solutions"

%% \begin{frame}
%%   \begin{itemize}
%%   \item convolutional neural networks (CNN)
%%   \item recurrent neural networks (RNN) with
%%   \item long short-term memory cells (LSTM)
%%   \end{itemize}

%%   "For the prediction of a continuous output, a single linear output
%%   neuron is typically used."
%% \end{frame}

%% \begin{frame}
%%   \frametitle{Convolutional Neural Network}
%%   \begin{itemize}
%%   \item "In convolutional neural networks (CNNs) information also flows
%%     only from the input to the output, layer by layer. They are however
%%     not fully connected, but instead slide a filter (a set of weights)
%%     over the input that feeds into a different neuron in the next layer
%%     each time it is moved as illustrated in"
%%   \end{itemize}
%% \end{frame}


%% \begin{frame}
%%   \frametitle{Links}
%%   \url{http://www.emergentmind.com/neural-network}
%%   \url{http://playground.tensorflow.org}
%% \end{frame}

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \section{Example applications in biology}
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \begin{frame}{}
%%    \tableofcontents[currentsection]
%% \end{frame}

% Sequences
% One-hot encoding
% - Represent categorial feature as vectors

\end{document}
